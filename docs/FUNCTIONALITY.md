# ðŸŽ¯ MCP Evaluation System - Complete Functionality Overview

## ðŸ“‹ Core Functionality

### ðŸ”§ **What It Does**
â€¢ **Tests MCP (Model Context Protocol) servers** with AI agents
â€¢ **Compares performance** between Claude and OpenCode agents
â€¢ **Stores evaluation results** in time-series database (InfluxDB/SQLite)
â€¢ **Tracks costs, timing, and success rates** for each evaluation
â€¢ **Generates comparative analytics** and statistics
â€¢ **Manages session continuity** across multiple prompts

### ðŸ¤– **Supported Agents**
â€¢ **Claude Code** (via Anthropic API)
  - Models: `sonnet`
  - Features: Cost tracking, JSON output, session management
  - Requires: `--skip-permissions` for automation
â€¢ **OpenCode** (multiple backends)
  - Models: GitHub Copilot (Claude 3.5, GPT-4o and more)
  - Features: Free usage, detailed logging, multiple model access
  - No permission requirements

## ðŸ“¥ **Input Types**

### ðŸ—‚ï¸ **Prompt Files** (`.md` format)
```yaml
---
id: 1
complexity: "low|medium|high"
target_mcp: ["node-hardware-mcp", "other-mcp"]
description: "What this prompt tests"
timeout: 60
expected_tools: ["MCP", "Read", "Write"]
tags: ["hardware", "discovery"]
---

# Prompt Content
Instructions for the AI agent...
```

### âš™ï¸ **Command Parameters**
â€¢ **Prompt Selection**: Single ID (`run 1`), batch (`batch 1 2 3`), all (`run-all`)
â€¢ **Agent Selection**: `--agent claude|opencode|both`
â€¢ **Model Selection**: `--claude-model haiku`, `--opencode-model "github-copilot/gpt-4o"`
â€¢ **Backend Selection**: `--backend influxdb|sqlite`
â€¢ **Session Management**: `--continue-session`, `--continue-sessions`
â€¢ **Filters**: `--complexity low`, `--mcp-target node-hardware-mcp`

### ðŸ“ **Configuration Files** (Optional)
â€¢ `evaluation_config_influxdb.yaml` - InfluxDB settings
â€¢ `evaluation_config_sqlite.yaml` - SQLite settings
â€¢ `opencode.json` - MCP server configuration

## ðŸ“¤ **Output Types**

### ðŸ–¥ï¸ **Terminal Output**
```
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ âœ… Claude Evaluation - Prompt 1 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Session ID: eval_prompt001_1756832722                                                          â”‚
â”‚ Success: True                                                                                  â”‚
â”‚ Response: ## System Hardware Discovery Summary...                                             â”‚
â”‚ Cost: $0.2376                                                                                 â”‚
â”‚ Duration: 46.2s                                                                               â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
```

### ðŸ“Š **Statistics Dashboard**
```
Evaluation Statistics
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”“
â”ƒ Metric                â”ƒ Value   â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”©
â”‚ Total Sessions        â”‚ 6       â”‚
â”‚ Comparative Sessions  â”‚ 2       â”‚
â”‚ Database Size (MB)    â”‚ 0.04    â”‚
â”‚ Average Cost (USD)    â”‚ $0.2376 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ðŸ—„ï¸ **Database Storage**
â€¢ **InfluxDB** (default): Time-series data with tags and fields
â€¢ **SQLite** (fallback): Relational data with JSON columns
â€¢ **Fields**: session_id, agent_type, model, success, cost, duration, response

### ðŸ“‹ **Comparative Results**
```
Comparative Evaluation - Prompt 1
Session ID: eval_prompt001_1756832005

Claude Code Result:
âœ… Success: True, Cost: $0.1351, Duration: 37.8s

OpenCode Result:  
âœ… Success: True, Duration: 29.7s

Comparison Summary:
âœ… Both agents succeeded
ðŸ’° Claude Code cost: $0.1351
```

## ðŸš€ **Command Categories**

### ðŸŽ¯ **Single Evaluations**
```bash
# Basic single evaluations
uv run python -m mcp_evaluation run 1                                    # Prompt 1, default settings
uv run python -m mcp_evaluation run 1 --agent claude --skip-permissions  # Claude only
uv run python -m mcp_evaluation run 1 --agent opencode                   # OpenCode only
uv run python -m mcp_evaluation run 1 --agent both --skip-permissions    # Both agents (comparative)

# Custom models
uv run python -m mcp_evaluation run 1 --agent claude --claude-model haiku --skip-permissions
uv run python -m mcp_evaluation run 1 --agent opencode --opencode-model "github-copilot/gpt-4o"
uv run python -m mcp_evaluation run 1 --agent both --claude-model opus --opencode-model "opencode/llama-3.1-70b" --skip-permissions
```

### ðŸ“¦ **Batch Operations**
```bash
# Run all prompts
uv run python -m mcp_evaluation run-all --agent both --skip-permissions

# Run specific prompts
uv run python -m mcp_evaluation batch 1 2 3 --agent both --skip-permissions

# With session continuity (context preserved between prompts)
uv run python -m mcp_evaluation batch 1 2 3 --continue-sessions --agent claude --skip-permissions

# Filtered by complexity or MCP target
uv run python -m mcp_evaluation run-all --complexity low --agent both --skip-permissions
uv run python -m mcp_evaluation run-all --mcp-target node-hardware-mcp --agent both --skip-permissions
```

### ðŸ“ˆ **Analytics & Export**
```bash
# View statistics
uv run python -m mcp_evaluation stats
uv run python -m mcp_evaluation stats --backend sqlite

# Show specific results
uv run python -m mcp_evaluation results 1 2 3

# Export data
uv run python -m mcp_evaluation export --output evaluation_results --format json
```

### ðŸ”§ **Setup & Management**
```bash
# Initialize infrastructure
uv run python -m mcp_evaluation setup

# Setup InfluxDB (Docker)
./scripts/setup_influxdb.sh

# Validate setup
uv run python -m mcp_evaluation setup --config evaluation_config_influxdb.yaml
```

## ðŸ“‹ **Data Flow**

### ðŸ”„ **Evaluation Process**
1. **Load prompt** from `.md` file in `prompts/` directory
2. **Initialize agent** with specified model and configuration
3. **Create session** with unique ID format: `eval_prompt{ID:03d}_{timestamp}`
4. **Execute evaluation** with timeout and retry logic
5. **Capture results** (success, response, cost, timing, tool usage)
6. **Store in database** (InfluxDB time-series or SQLite relational)
7. **Display formatted output** with color-coded success/failure

### ðŸ”— **Session Management**
â€¢ **Individual Sessions**: Each prompt gets unique session ID
â€¢ **Comparative Sessions**: Same base session ID for both agents
â€¢ **Session Continuity**: Context preserved across multiple prompts
â€¢ **Session Format**: `eval_prompt{ID:03d}_{timestamp}`
â€¢ **UUID Conversion**: Claude converts to deterministic UUID format

## ðŸŽ¯ **Use Cases**

### ðŸ§ª **MCP Server Testing**
â€¢ Verify MCP servers work with different agents
â€¢ Test hardware discovery, file operations, API calls
â€¢ Validate MCP protocol compliance
â€¢ Regression testing for MCP updates

### âš–ï¸ **Agent Comparison**
â€¢ Compare Claude vs OpenCode on identical tasks
â€¢ Evaluate different models (Haiku vs GPT-4o vs Llama)
â€¢ Analyze cost vs performance trade-offs
â€¢ A/B testing for prompt effectiveness

### ðŸ“Š **Performance Analysis**
â€¢ Track success rates over time
â€¢ Monitor evaluation costs and trends
â€¢ Identify optimal model combinations
â€¢ Generate comparative performance reports

### ðŸ” **Quality Assurance**
â€¢ Validate MCP integrations before deployment
â€¢ Automated evaluation pipelines
â€¢ Continuous integration testing
â€¢ Performance benchmarking

## ðŸ’¾ **Storage & Persistence**

### ðŸ“ˆ **InfluxDB (Default)**
â€¢ **Type**: Time-series database optimized for analytics
â€¢ **Tags**: agent_type, model, complexity, mcp_target, prompt_id
â€¢ **Fields**: success, cost_usd, duration_seconds, response_length, token_count
â€¢ **Features**: Retention policies, downsampling, real-time queries
â€¢ **Setup**: Docker-based with automated initialization

### ðŸ—ƒï¸ **SQLite (Fallback)**
â€¢ **Type**: Local file-based relational database
â€¢ **Schema**: Sessions table with JSON columns for flexible data
â€¢ **Features**: SQL queries for custom analysis, portable database file
â€¢ **Use Case**: Development, testing, offline environments

### ðŸ”„ **Data Retention**
â€¢ All evaluation results preserved indefinitely
â€¢ Session history maintained with full context
â€¢ Cost tracking across time periods
â€¢ Comparative analysis over multiple runs
â€¢ Export capabilities for external analysis

## ðŸ› ï¸ **Technical Architecture**

### ðŸ—ï¸ **Core Components**
â€¢ **CLI Module** (`cli.py`): Command-line interface and argument parsing
â€¢ **Evaluation Engine** (`evaluation_engine.py`): Core orchestration logic
â€¢ **Unified Agent** (`unified_agent.py`): Agent abstraction layer
â€¢ **Session Manager** (`session_manager.py`): Database operations
â€¢ **Prompt Loader** (`prompt_loader.py`): Markdown prompt parsing

### ðŸ”Œ **Agent Integration**
â€¢ **Claude Code**: JSON API integration with cost tracking
â€¢ **OpenCode**: Multiple model support via configuration
â€¢ **Unified Interface**: Common evaluation API for both agents
â€¢ **Error Handling**: Graceful degradation and retry logic

### ðŸ—„ï¸ **Database Backends**
â€¢ **Factory Pattern**: Dynamic backend selection
â€¢ **InfluxDB Manager**: Time-series operations with proper tagging
â€¢ **SQLite Manager**: Relational operations with JSON flexibility
â€¢ **Automatic Fallback**: SQLite backup if InfluxDB unavailable

## ðŸ“‹ **Available Models**

### ðŸ§  **Claude Models**
| Model | Description | Cost | Speed | Use Case |
|-------|-------------|------|-------|----------|
| `sonnet` | Claude 3.5 Sonnet (default) | Medium | Fast | General purpose |


### ðŸ¤– **OpenCode Models**
| Model | Description | Cost | Provider |
|-------|-------------|------|----------|
| `github-copilot/claude-3.5-sonnet` | Claude 3.5 via GitHub (default) | Free | GitHub Copilot |
| `github-copilot/gpt-4o` | GPT-4o via GitHub | Free | GitHub Copilot |


## ðŸŽ›ï¸ **Configuration Options**

### ðŸ“ **YAML Configuration**
```yaml
# Agent configurations
claude_config:
  type: "claude"
  model: "sonnet"
  output_format: "json"
  timeout: 60
  allowed_tools: ["Bash", "Edit", "Read", "Write", "MCP"]
  dangerously_skip_permissions: true

opencode_config:
  type: "opencode" 
  model: "github-copilot/claude-3.5-sonnet"
  timeout: 60
  enable_logs: true

# Evaluation settings
prompts_dir: "prompts"
timeout_seconds: 60
max_retries: 3
session_id_format: "eval_prompt{prompt_id:03d}_{timestamp}"

# Database backend
backend: "influxdb"
influxdb_url: "http://localhost:8086"
influxdb_token: "mcp-evaluation-token"
influxdb_org: "mcp-evaluation"
influxdb_bucket: "evaluation-sessions"
```

### ðŸ³ **Docker Configuration**
```bash
# InfluxDB Docker setup
docker run -d \
  --name influxdb \
  -p 8086:8086 \
  -v influxdb-data:/var/lib/influxdb2 \
  -e DOCKER_INFLUXDB_INIT_MODE=setup \
  -e DOCKER_INFLUXDB_INIT_USERNAME=admin \
  -e DOCKER_INFLUXDB_INIT_PASSWORD=adminpassword \
  -e DOCKER_INFLUXDB_INIT_ORG=mcp-evaluation \
  -e DOCKER_INFLUXDB_INIT_BUCKET=evaluation-sessions \
  influxdb:2.7
```

## ðŸ” **Monitoring & Analytics**

### ðŸŒ **InfluxDB Web Dashboard**
```bash
# Access InfluxDB UI directly in browser
http://localhost:8086

# Default credentials (from scripts/setup_influxdb.sh)
Username: admin
Password: adminpassword
Organization: mcp-evaluation
Bucket: evaluation-sessions
```

### ðŸ“Š **Dashboard Navigation**
â€¢ **Data Explorer**: Query and visualize evaluation data
â€¢ **Dashboards**: Create custom monitoring dashboards
â€¢ **Tasks**: Set up automated data processing
â€¢ **Settings**: Manage users, tokens, and buckets

### ðŸ” **Direct InfluxDB Queries**
```sql
-- View all evaluation results
from(bucket: "evaluation-sessions")
  |> range(start: -7d)
  |> filter(fn: (r) => r["_measurement"] == "evaluation_results")

-- Success rate by agent
from(bucket: "evaluation-sessions")
  |> range(start: -30d)
  |> filter(fn: (r) => r["_measurement"] == "evaluation_results")
  |> filter(fn: (r) => r["_field"] == "success")
  |> group(columns: ["agent_type"])
  |> mean()

-- Cost analysis over time
from(bucket: "evaluation-sessions")
  |> range(start: -30d)
  |> filter(fn: (r) => r["_measurement"] == "evaluation_results")
  |> filter(fn: (r) => r["_field"] == "cost_usd")
  |> aggregateWindow(every: 1d, fn: sum)
```

### ðŸ“± **Quick Dashboard Setup**
1. Open http://localhost:8086 in browser
2. Login with admin/adminpassword
3. Go to "Dashboards" â†’ "Create Dashboard"
4. Add cells with evaluation queries
5. Save dashboard for monitoring

### ðŸ“Š **Grafana Dashboard Integration**
```bash
# Setup Grafana with InfluxDB data source
docker run -d \
  --name grafana \
  -p 3000:3000 \
  -v grafana-data:/var/lib/grafana \
  -e "GF_SECURITY_ADMIN_PASSWORD=admin" \
  grafana/grafana:latest

# Access Grafana Dashboard
http://localhost:3000
Username: admin
Password: admin

# Pre-configured MCP Evaluation Dashboard:
http://localhost:3000/d/mcp-evaluation-main/mcp-evaluation-system-dashboard
```

### ðŸ”— **InfluxDB Data Source Configuration**
```yaml
# Grafana InfluxDB Data Source Settings
URL: http://host.docker.internal:8086
Database: evaluation-sessions
User: admin
Password: adminpassword
HTTP Method: GET

# Query Language: Flux
Organization: mcp-evaluation
Default Bucket: evaluation-sessions
Token: mcp-evaluation-token
```

### ðŸ“Š **Pre-built Grafana Queries**
```sql
# Success Rate Over Time
from(bucket: "evaluation-sessions")
  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)
  |> filter(fn: (r) => r["_measurement"] == "evaluation_results")
  |> filter(fn: (r) => r["_field"] == "success")
  |> aggregateWindow(every: 1h, fn: mean, createEmpty: false)

# Cost Analysis by Agent
from(bucket: "evaluation-sessions")
  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)
  |> filter(fn: (r) => r["_measurement"] == "evaluation_results")
  |> filter(fn: (r) => r["_field"] == "cost_usd")
  |> group(columns: ["agent_type"])
  |> sum()

# Response Time Distribution
from(bucket: "evaluation-sessions")
  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)
  |> filter(fn: (r) => r["_measurement"] == "evaluation_results")
  |> filter(fn: (r) => r["_field"] == "duration_seconds")
  |> histogram(bins: [0.0, 10.0, 30.0, 60.0, 120.0, 300.0])
```

### ðŸ“Š **Built-in Statistics**
â€¢ Total sessions and comparative sessions
â€¢ Success rate by agent and model
â€¢ Average costs and duration
â€¢ Database size and recent activity
â€¢ Agent distribution and prompt complexity

### ðŸ“ˆ **Custom Queries**
```python
# Example InfluxDB queries
from influxdb_client import InfluxDBClient

client = InfluxDBClient(url="http://localhost:8086", token="your-token", org="mcp-evaluation")
query_api = client.query_api()

# Success rate by agent
query = '''
from(bucket: "evaluation-sessions")
  |> range(start: -30d)
  |> filter(fn: (r) => r["_measurement"] == "evaluation_results")
  |> group(columns: ["agent_type"])
  |> mean(column: "_value")
'''
```

### ðŸŽ¯ **Performance Metrics**
â€¢ Response time percentiles
â€¢ Cost per successful evaluation
â€¢ Token efficiency ratios
â€¢ Error rate trends
â€¢ Model comparison matrices

---

**ðŸŽ¯ Bottom Line**: Complete MCP testing automation with AI agent comparison, comprehensive analytics, and production-ready monitoring! ðŸš€
